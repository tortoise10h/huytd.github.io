<html>
    <head>
        <title> Generalized Linear Models | Huy's Blog</title>
        <meta charset="utf-8">
        <meta http-equiv="content-type" content="text/html;"><meta name=viewport content="initial-scale=1.0 maximum-scale=1.0">
        
        <link href="../css/inconsolata.css" rel="stylesheet" type="text/css">
        <link href="../css/theme.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="../css/highlight/tomorrow.css">
        <link rel="stylesheet" href="../css/fontello.css">
        <script src="../js/highlight.pack.js"></script>
        <script src="../js/autosizing.js"></script>
        <script>
        hljs.initHighlightingOnLoad();
        </script>
    </head>
    <body>
      <div class="notice">N·ªôi dung tr√™n blog n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t l·∫ßn cu·ªëi v√†o l√∫c <b>11:50 02/05/2017</b>.<br/>T·ª´ gi·ªù blog s·∫Ω chuy·ªÉn sang nh√† m·ªõi l√† <a href="https://thefullsnack.com">https://thefullsnack.com</a>. Mong c√°c b·∫°n gh√© ch∆°i <i class="icon icon-emo-coffee"></i></div>
        <div class="header">
            <a href="/"><i class="icon icon-emo-coffee"></i> Huy's Blog</a>
        </div>
        <div class="container">
            <div class="main">
                <h1 id="generalized-linear-models">Generalized Linear Models</h1>
<hr>
<p><strong>Notice:</strong> There so many mathematics concept need to be covered in this chapter, I tried to put them all in just one post, that&#39;s why it&#39;s so damn long, but please read it carefully one by one and just don&#39;t skip. Good luck!</p>
<hr>
<p>To understand <strong>Generalized Linear Models</strong>, we need to talk about the <strong>probabilistic intepretation</strong> of a <strong>model</strong> at first.</p>
<h2 id="probabilistic-interpretation">Probabilistic Interpretation</h2>
<p>Let&#39;s take a look at how we can represent our <a href="https://github.com/huytd/til/blob/master/machine-learning/linear-regression.md">last</a> <a href="https://github.com/huytd/til/blob/master/machine-learning/logistic-regression.pdf">two</a> problems in <strong>probabilistic</strong> form.</p>
<h3 id="linear-regression-models">Linear Regression Models</h3>
<p>In linear regression, we have the target and input values related via this equation:</p>
<pre class="math">
$y^{(i)} = \theta_{0} + \theta_{1}x^{(i)}_{1} + ... + \theta_{n}x^{(i)}_{n}$
</pre>

<p>Or we can write in matrix notation as:</p>
<pre class="math">
$y^{(i)} = \theta^{T}x^{(i)}$
</pre>

<p>To be percise, sometimes we have some unmodelled effects (something useful but we left out of the regression) and we should capture them, so we can add some error term to the equation above to do that:</p>
<pre class="math">
$y^{(i)} = \theta^{T}x^{(i)} + \epsilon^{(i)}$
</pre>

<p>From now on, let&#39;s assume that the <code>$\epsilon^{(i)}$</code> are distributed <strong>IID</strong> (independently and identically distributed) according to <a href="https://en.m.wikipedia.org/wiki/Normal_distribution">Normal Distribution</a> with <strong>zero mean</strong> (<code>$\mu = 0$</code>) and some <strong>variance</strong> <code>$\sigma^{2}$</code></p>
<pre class="math">
$\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^{2})$
</pre>

<p>So we have the <strong>probability destiny</strong> (distribution) of <code>$\epsilon^{(i)}$</code> is:</p>
<pre class="math">
$\rho(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(\epsilon^{(i)})^{2}}{2\sigma^{2}})$
</pre>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/2000px-Normal_Distribution_PDF.svg.png" alt=""></p>
<p>But hold on! We don&#39;t <em>really care</em> about this, so don&#39;t pay too much attention to this equation.</p>
<p>What we need to know is the <strong>distribution of</strong> <code>$y^{(i)}$</code> given by <code>$x^{(i)}$</code> and parameter <code>$\theta$</code> with the co-effect from <code>$\epsilon^{(i)}$</code>:</p>
<pre class="math">
$\rho(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}})$
</pre>

<p>Or shorten it a bit with:</p>
<div class="box-red">
<pre class="math">
$y^{(i)}|x^{(i)};\theta \sim \mathcal{N}(\theta^{T}x^{(i)}, \sigma^{2})$
</pre>
</div>

<h4 id="the-likelihood">The Likelihood</h4>
<p>When it comes to <strong>likelihood</strong>, many people get confused with the <strong>probability</strong>. Let&#39;s compare the two terms:</p>
<ul>
<li><strong>Probability</strong>: is the <em>unknown outcome</em> <code>$\rho$</code> to be predicted based on <em>known parameter</em> <code>$\theta$</code>, notated by: <code>$\rho(x|\theta)$</code></li>
<li><strong>Likelihood</strong>: is the <em>unknown parameter</em> <code>$\theta$</code> to be estimated based on <em>known outcome</em> <code>$\rho$</code>, notated by: <code>$L(\theta|x)$</code> itself, and with the known outcome, we have: <code>$L(\theta|x) = \rho(x|\theta)$</code></li>
</ul>
<div class="box-white" style="padding: 10px;">

<div><strong>Example:</strong></div>

<p>Let&#39;s take a closer look with the two terms with the <strong>coin flipping</strong> example. When we flip a coin, it will gives us a <em>head</em> or a <em>tail</em> face. So, we have $\theta$ is probability of getting <em>heads</em> (parameter), and it will be <strong>0.5</strong> (50% head, 50% tail). And $\vec{x}$ is the outcome, tell us the flip results $\vec{x} = [H, H, H, T, T, H]$ </p>
<p>So, the <strong>probability</strong> of outcome given parameter is: </p>
<pre class="math">
$\rho(\vec{x}=[H, H, H, T, T, H] | \theta=0.5) = 0.5^{6} = 0.016$
</pre>

<p>The <strong>likelihood</strong> of parameter $\theta=0.5$ with given outcome is:</p>
<pre class="math">
$L(\theta=0.5|\vec{x}=[H, H, H, T, T, H]) = \rho(\vec{x}|\theta) = 0.016$
</pre>

<p><img src="img/coin-toss.png" alt=""></p>
<p></div></p>
<p><em>Note:</em> The likelihood is defined differently for discrete and continuos probability distribution, please <a href="https://en.m.wikipedia.org/wiki/Likelihood_function">check it on Wikipedia</a> for a detailed definition.</p>
<p>Back to the Linear Regression, given <strong>X</strong> (the <a href="https://en.m.wikipedia.org/wiki/Design_matrix">design matrix</a> which contains all the <code>$x^{(i)}$</code> values), and <code>$\theta$</code>, the probability of the data is given by <code>$\rho(\vec{y}|X;\theta)$</code>.</p>
<p>This probability is a <strong>function of $\vec{y}$ with a fixed value $\theta$</strong>. For optimization, we need this to be a <strong>function of $\theta$</strong> instead, this function can be called <strong>likelihood function</strong>:</p>
<pre class="math">
$L(\theta) = L(\theta;X;\vec{y}) = \rho(\vec{y}|X;\theta)$
</pre>

<p>Expanding this, we have:</p>
<div class="box-red">
<pre class="math">
$L(\theta) = \displaystyle\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})$
</pre>
</div>

<h4 id="maximum-likelihood">Maximum Likelihood</h4>
<p>In the coin flipping example above, we noticed that the <strong>likelihood</strong> reaching its maximum when <code>$\theta=0.6676$</code></p>
<p><img src="img/coin-toss-max.png" alt=""></p>
<p>When this happen, we could say: <code>$\theta = 0.6676$</code> maximize <code>$L(\theta)$</code>, and it also make the data model has the highest probability.</p>
<p>The process to find the best <code>$\theta$</code> that maximize the likelihood is called <strong>maximum likelihood estimation</strong></p>
<p>We can use any optimization algorithm to maximize likelihood, for example, we can use <strong>Gradient Descent Algorithm</strong>.</p>
<p>To maximize the likelihood, we can maximizing <code>$L(\theta)$</code> or maximizing any function of <code>$L(\theta)$</code>, and if we gonna use <strong>Gradient Descent</strong>, it&#39;s better to do the optimization with <strong>log likelihood</strong> (<code>$\ell(\theta)$</code>) because it will be easier to find its <strong>derivative</strong>.</p>
<div class="box-red">
<pre class="math">
$\ell(\theta) = m\log\frac{1}{\sqrt{2\pi\sigma}}-\frac{1}{\sigma^{2}}\frac{1}{2}\displaystyle\sum_{i=1}^{m}(y^{(i)}-\theta^{T}x^{(i)})^{2}$
</pre>
</div>

<h3 id="logistic-regression-models">Logistic Regression Models</h3>
<p>For Logistic Regression, our hypothesis function is:</p>
<pre class="math">
$h_{\theta}(x) = g(\theta^{T}x)=\frac{1}{1 + e^{-\theta^{T}x}}$
</pre>

<p>Its derivative should be:</p>
<pre class="math">
$g'(z) = \frac{d}{dz}\frac{1}{1 + e^{-z}} = g(z)(1 - g(z))$
</pre>

<p>So, as above, let&#39;s talk about the probabilistic represent of Linear Regression Models.</p>
<h4 id="bernoulli-distribution">Bernoulli Distribution</h4>
<p>We noticed that the probability distribution of a Logistic Regression Models could be defined by <a href="https://en.m.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli Distribution</a></p>
<pre class="math">
$\rho(y|x;\theta)=
\begin{cases}
  h_{\theta}(x) & \text{if y = 1} \\
  1 - h_{\theta}(x) & \text{if y = 0}
\end{cases}
$
</pre>

<p>Or write compactly as:</p>
<pre class="math">
$\rho(y|x;\theta) = (h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}$
</pre>

<h4 id="likelihood-and-log-likelihood">Likelihood and Log Likelihood</h4>
<p>Assuming that we have m training examples generated independently, the <strong>likelihood</strong> of parameters will be:</p>
<pre class="math">
$L(\theta) = \rho(\vec{y}|X;\theta) = \displaystyle\prod_{i=1}^{m} (h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}$
</pre>

<p>And here is the <strong>log likelihood</strong></p>
<div class="box-red">
<pre class="math">
$\ell(\theta) = \log{L(\theta)} = \displaystyle\sum_{i=1}^{m}y^{(i)}\log{h(x^{(i)})}+(1-y^{(i)})\log{(1-h(x^{(i)}))}$
</pre>
</div>

<p>Then you can use <strong>Gradient Descent</strong> to estimate $\theta$ just same as Linear Regression problem above.</p>
<h2 id="generalized-linear-models">Generalized Linear Models</h2>
<p>So, in the two problems described above, we have <code>$y|x;\theta \sim \mathcal{N}(\mu,\sigma^{2})$</code> as a probabilistic representation of a regression model, and another one is <code>$y|x;\theta \sim \text{Bernoulli}(\phi)$</code> (which $\phi$ is a function of $x$ and $\theta$) as a probabilistic representation of a classification model.</p>
<p>Those two are the <strong>special cases</strong> of a <em>broader family of models</em>, which called <strong>Generalized Linear Models</strong> (GLMs)</p>
<h3 id="the-exponential-family">The exponential family</h3>
<p><strong>Exponential Function</strong> is a function of the form:</p>
<pre class="math">
$f(x)=b^{x}$
</pre>

<p><img src="img/exponential-function.png" alt=""></p>
<p>Its distribution can be written in the form:</p>
<pre class="math">
$\rho(y;\eta) = b(y)exp(\eta^{T}T(y) - a(\eta))$
</pre>

<p><code>$\eta$</code> is called the <strong>natural parameter</strong> (a.k.a <strong>canonical parameter</strong>) of the distribution.</p>
<p><code>$T(y)$</code> is the <strong>sufficient statistic</strong> (in this distribution, it can be $T(y) = y$)</p>
<p><code>$a(\eta)$</code> is the <strong>log partition function</strong></p>
<p>A <em>fixed</em> choice of <strong>T</strong>, <strong>a</strong> and <strong>b</strong> defines a <strong>family</strong> (or set) of distributions that parameterized by <code>$\eta$</code>. As we vary <code>$\eta$</code>, we get the different distribution within the faimily.</p>
<p><img src="img/vary-eta.png" alt=""></p>
<h4 id="special-case-of-exponential-bernoulli-distribution">Special case of exponential: Bernoulli Distribution</h4>
<p>The Bernoulli distribution with mean $\phi$ written by <code>$\text{Bernoulli}(\phi)$</code>. We have:</p>
<pre class="math">
$\rho(y;\phi) = \phi^{y}(1-\phi)^{1-y} = exp((\log{(\frac{\phi}{1-\phi})})y + \log{(1 - \phi)})$
</pre>

<p>The choice of <strong>T</strong>, <strong>a</strong> and <strong>b</strong> here is:</p>
<pre class="math">
$\begin{eqnarray*}
\eta & = & \log{(\phi / (1 - \phi))} \\
\phi & = & 1/(1 + e^{-\eta}) \\
T(y) & = & y \\
a(\eta) & = & -\log{(1 - \phi)} = \log{(1 + e^{\eta})} \\
b(y) & = & 1
\end{eqnarray*}$
</pre>

<p>We may noticed that the $\phi$ is obtained by invert the definition of $\eta$ and solving it, it will be a similar function with sigmoid function.</p>
<h4 id="another-case-of-exponential-normal-distribution">Another case of exponential: Normal Distribution</h4>
<p>Remember that when deriving linear regression, the value of <code>$\sigma^{2}$</code> has no effect on the final choice of <code>$\theta$</code> and $h_{\theta}(x)$. So, let&#39;s set <code>$\sigma^{2} = 1$</code>. We then have:</p>
<pre class="math">
$
\begin{eqnarray*}
\rho(y;\mu) & = & \frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}(y - \mu)^2)} \\
            & = & \frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}y^{2})}\exp{(\mu y-\frac{1}{2}\mu^{2})}
\end{eqnarray*}
$
</pre>

<p>So, the choice of <strong>T</strong>, <strong>a</strong> and <strong>b</strong> here is:</p>
<pre class="math">
$\begin{eqnarray*}
\eta & = & \mu \\
T(y) & = & y \\
a(\eta) & = & \mu^{2} / 2 = \eta^{2} / 2 \\
b(y) & = & (1 / \sqrt{2\pi})\exp{(-y^{2}/2)}
\end{eqnarray*}$
</pre>

<p>Next, let&#39;s talk about the general approach to constructing GLM.</p>
<h3 id="bohf">BOHF</h3>
<h2 id="bleh">BLEH</h2>

                <div class="copyright">
                B·∫°n ƒë∆∞·ª£c t√πy √Ω b·∫•m like, tr√≠ch d·∫´n ho·∫∑c copy, post l·∫°i, nh∆∞ng vui l√≤ng ghi r√µ ngu·ªìn v√† t√°c gi·∫£ v√† kh√¥ng l√†m thay ƒë·ªïi n·ªôi dung b√†i vi·∫øt. N·∫øu kh√¥ng l√†m v·∫≠y, m√¨nh hy v·ªçng t·ª´ nay v·ªÅ sau b·∫°n s·∫Ω lu√¥n c·∫£m th·∫•y c·∫Øn r·ª©t l∆∞∆°ng t√¢m, ƒÉn kh√¥ng ngon, ng·ªß kh√¥ng y√™n. üòÜ
                </div>
                <!-- Comment -->
                <div class="comments">
                  <div id="comment-loading" class="loading"></div>
                  <div id="login-box" class="login">
                    B·∫°n c·∫ßn <button onclick="login()">Login</button> ƒë·ªÉ comment
                  </div>
                  <div id="comment-box" class="comment-input">
                    <div class="avatar">
                      <img id="user-avatar" src="" width="32" height="32"/>
                    </div>
                    <div class="input">
                      <textarea id="comment-content" onkeyup="autoSizing(this)" onkeydown="submitComment(event)" placeholder="Comment g√µ v√†o ƒë√¢y :D"></textarea>
                      <span>G√µ xong nh·∫•n <kbd>Ctrl</kbd> + <kbd>Enter</kbd> ƒë·ªÉ g·ª≠i.</span>
                    </div>
                  </div>
                  <ul id="comment-list" class="comment-list"></ul>
                </div>
                <!-- End Comment -->
            </div>
        </div>
	    <div class="footer">
        <p><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img src="https://thefullsnack.com/img/by-nc-nd.png" height="15" /></a></p>
            <p>Created with <a href="http://github.com/huytd/azeroth-js">azeroth.js</a></p>
            <div class="social">
                <a target="_blank" href="http://facebook.com/kingbazoka"><i class="icon-facebook-squared"></i></a>
                <a target="_blank" href="http://twitter.com/huydotnet"><i class="icon-twitter-squared"></i></a>
                <a target="_blank" href="http://github.com/huytd"><i class="icon-github-squared"></i></a>
                <a target="_blank" href="https://thefullsnack.com"><i class="icon-emo-coffee"></i></a>
            </div>
        </div>
        <script type="text/javascript" async src="https://cdn.rawgit.com/mathjax/MathJax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            skipTags: ["script","noscript","style","textarea"]
          }
        });
        </script>
        <script src="https://www.gstatic.com/firebasejs/3.8.0/firebase.js"></script>
        <script>
          // Initialize Firebase
          var config = {
            apiKey: "AIzaSyA1PnIAlJJ-U6iQJMNnCOknBuziBqGMYcY",
            authDomain: "huys-blog-comment.firebaseapp.com",
            databaseURL: "https://huys-blog-comment.firebaseio.com",
            projectId: "huys-blog-comment",
            storageBucket: "huys-blog-comment.appspot.com",
            messagingSenderId: "317330376829"
          };
          firebase.initializeApp(config);
        </script>
        <script>
          let provider = new firebase.auth.GoogleAuthProvider();
          let auth = firebase.auth();
          let currentUser = null;
          let postCommentURL = 'posts/generalized-linear-models/comments';

          auth.onAuthStateChanged(function(user) {
            document.getElementById("comment-loading").style.display = "none";
            if (user) {
              currentUser = user;
              // Logged in
              document.getElementById("login-box").style.display = "none";
              document.getElementById("comment-box").style.display = "flex";
              document.getElementById("user-avatar").setAttribute("src", user.photoURL);
            } else {
              // Not login yet
              document.getElementById("comment-box").style.display = "none";
              document.getElementById("login-box").style.display = "block";
            }
          });

          const login = function() {
            auth.signInWithPopup(provider)
              .then(function(result) {
              }).catch(function(err) {
              });
          };

          const encodeHTML = function(s) {
            return s.replace(/</g, '&lt;').replace(/"/g, '&quot;').replace(/>/g, '&gt;');
          };

          const saveNewComment = function(comment) {
            if (!currentUser) {
              return;
            }
            let commentData = {
              user: currentUser.displayName,
              avatar: currentUser.photoURL,
              time: (new Date()).getTime(),
              message: encodeHTML(comment)
            };
            database.ref(postCommentURL).push(commentData);
            document.getElementById("comment-content").value = "";
          };

          const submitComment = function(e) {
            let keyCode = e.which || e.keyCode;
            let ctrlCode = e.ctrlKey || e.metaKey;
            if (keyCode === 13 && ctrlCode) {
              let comment = document.getElementById("comment-content").value;
              saveNewComment(comment);
            }
          };

          const filterURLinComment = function(comment) {
            return comment.replace(/(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})/g, "<a target='_blank' rel='noopener noreferrer' href='$1'>$1</a>");
          };

          const addNewComment = function(user, avatar, time, comment) {
            let commentFiltered = encodeHTML(comment);
            commentFiltered = filterURLinComment(commentFiltered);
            let d = new Date(time);
            let commentTime = d.toLocaleTimeString() + ' ' + d.toLocaleDateString();
            let html = '<div class="avatar">' +
              '   <img src="' + avatar + '" width="32" height="32"/>' +
              ' </div>' +
              '<div class="comment">' +
              '  <div class="metadata"><b>' + user + '</b> l√∫c <span>' + commentTime + '</span></div>' +
              '  <div class="content">' + commentFiltered
              '  </div>' +
              '</div>';
            let li = document.createElement('li');
            li.innerHTML = html;
            document.getElementById("comment-list").append(li);
          };

          let database = firebase.database();
          let posts = database.ref(postCommentURL).orderByChild('time');
          posts.on('child_added', function(data) {
            addNewComment(data.val().user, data.val().avatar, data.val().time, data.val().message);
          });
        </script>
    </body>
</html>
